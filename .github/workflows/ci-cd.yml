name: CI/CD Pipeline

on:
  push:
    branches:
      - main

env:
  AWS_REGION: us-east-1
  CLUSTER_NAME: infoline-eks-cluster
  ECR_BACKEND_REPO: 903479130308.dkr.ecr.us-east-1.amazonaws.com/hello-springboot
  ECR_FRONTEND_PUBLIC_REPO: 903479130308.dkr.ecr.us-east-1.amazonaws.com/frontend-public
  ECR_FRONTEND_ADMIN_REPO: 903479130308.dkr.ecr.us-east-1.amazonaws.com/frontend-admin

permissions:
  id-token: write
  contents: read

jobs:
  backend:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: java-api

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up JDK
        uses: actions/setup-java@v3
        with:
          java-version: '17'
          distribution: 'temurin'

      - name: Build Spring Boot
        run: mvn clean package -DskipTests

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: arn:aws:iam::903479130308:role/GitHubActionsOIDCRole
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to ECR
        run: |
          aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $ECR_BACKEND_REPO

      - name: Build and Push Docker image
        run: |
          docker build -t hello-springboot:latest .
          docker tag hello-springboot:latest $ECR_BACKEND_REPO:latest
          docker push $ECR_BACKEND_REPO:latest

  frontend-public:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: angular/frontend-public

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '20'

      - name: Install dependencies
        run: npm install

      - name: Build Angular (frontend-public)
        run: npm run build -- --configuration production --project frontend-public

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: arn:aws:iam::903479130308:role/GitHubActionsOIDCRole
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to ECR
        run: |
          aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $ECR_FRONTEND_PUBLIC_REPO

      - name: Build and Push Docker image
        run: |
          docker build -t frontend-public:latest .
          docker tag frontend-public:latest $ECR_FRONTEND_PUBLIC_REPO:latest
          docker push $ECR_FRONTEND_PUBLIC_REPO:latest

  frontend-admin:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: angular/frontend-admin

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Node.js
        uses: actions/setup-node@v3
        with:
          node-version: '20'

      - name: Install dependencies
        run: npm install

      - name: Build Angular (frontend-admin)
        run: npm run build -- --configuration production --project frontend-admin

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: arn:aws:iam::903479130308:role/GitHubActionsOIDCRole
          aws-region: ${{ env.AWS_REGION }}

      - name: Login to ECR
        run: |
          aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $ECR_FRONTEND_ADMIN_REPO

      - name: Build and Push Docker image
        run: |
          docker build -t frontend-admin:latest .
          docker tag frontend-admin:latest $ECR_FRONTEND_ADMIN_REPO:latest
          docker push $ECR_FRONTEND_ADMIN_REPO:latest

  deploy:
    runs-on: ubuntu-latest
    needs: [backend, frontend-public, frontend-admin]

    env:
      HEALTH_TIMEOUT: 180

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: arn:aws:iam::903479130308:role/GitHubActionsOIDCRole
          aws-region: ${{ env.AWS_REGION }}

      - name: Update kubeconfig (EKS)
        run: aws eks update-kubeconfig --region $AWS_REGION --name $CLUSTER_NAME

      # ---------- Terraform pour la supervision (EFK + CronJob) ----------
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.12.0

      - name: Tell Terraform where the kubeconfig is
        run: echo "TF_VAR_kubeconfig_path=$HOME/.kube/config" >> $GITHUB_ENV

      - name: Terraform Init (terraform/monitoring)
        working-directory: terraform/monitoring
        run: terraform init -input=false

      - name: Terraform Apply (monitoring only)
        working-directory: terraform/monitoring
        run: terraform apply -auto-approve -no-color

      # ---------- Déploiement de tes applications ----------
      - name: Deploy to EKS (apps)
        run: |
          set -euxo pipefail
          # Services (si présents dans ton repo)
          if [ -f terraform/kubernetes/java-api-service.yaml ]; then
            kubectl apply -f terraform/kubernetes/java-api-service.yaml --validate=false
          fi
          if [ -f terraform/kubernetes/frontend-public-service.yaml ]; then
            kubectl apply -f terraform/kubernetes/frontend-public-service.yaml --validate=false
          fi
          if [ -f terraform/kubernetes/frontend-admin-service.yaml ]; then
            kubectl apply -f terraform/kubernetes/frontend-admin-service.yaml --validate=false
          fi

          # Deployments
          kubectl apply -f terraform/kubernetes/java-api-deployment.yaml --validate=false
          kubectl apply -f terraform/kubernetes/frontend-public-deployment.yaml --validate=false
          kubectl apply -f terraform/kubernetes/frontend-admin-deployment.yaml --validate=false

      # ---------- Attendre que les déploiements soient prêts ----------
      - name: Wait for rollouts
        run: |
          set -euxo pipefail
          kubectl rollout status deployment/java-api -n default --timeout=${HEALTH_TIMEOUT}s
          kubectl rollout status deployment/frontend-public -n default --timeout=${HEALTH_TIMEOUT}s
          kubectl rollout status deployment/frontend-admin -n default --timeout=${HEALTH_TIMEOUT}s

      # ---------- Smoke tests (HTTP depuis un pod éphémère dans le cluster) ----------
      - name: Smoke tests
        run: |
          set -euo pipefail
          ns=default
          k8s_curl () {
            svc="$1"; path="$2"
            if kubectl get svc -n "$ns" "$svc" >/dev/null 2>&1; then
              echo "Testing http://$svc.$ns.svc.cluster.local$path"
              kubectl run "curl-$svc-$$" -n "$ns" --rm -i --restart=Never \
                --image=curlimages/curl:8.10.0 --quiet -- /bin/sh -lc \
                "code=\$(curl -s -o /dev/null -w '%{http_code}' http://$svc.$ns.svc.cluster.local$path || true); echo HTTP=\$code; test \"\$code\" = \"200\""
            else
              echo "Service $svc not found in $ns namespace -> skipping HTTP check"
            fi
          }
          # Adapte les noms de services si besoin
          k8s_curl java-api-service /actuator/health
          k8s_curl frontend-public-service /
          k8s_curl frontend-admin-service  /

      # ---------- Afficher l'URL externe de Kibana ----------
      - name: Show Kibana external address
        run: |
          set -e
          echo "Waiting for Kibana LoadBalancer external address..."
          for i in $(seq 1 60); do
            host=$(kubectl get svc -n monitoring kibana -o jsonpath='{.status.loadBalancer.ingress[0].hostname}' 2>/dev/null || true)
            ip=$(kubectl   get svc -n monitoring kibana -o jsonpath='{.status.loadBalancer.ingress[0].ip}' 2>/dev/null || true)
            url=""
            [ -n "$host" ] && url="http://$host:5601"
            [ -n "$ip"   ] && url="http://$ip:5601"
            if [ -n "$url" ]; then
              echo "::notice title=Kibana URL::$url"
              exit 0
            fi
            sleep 10
          done
          echo "Kibana LoadBalancer not ready yet; check later with: kubectl get svc -n monitoring kibana -o wide"
