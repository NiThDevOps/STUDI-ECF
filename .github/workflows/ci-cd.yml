name: CI/CD Pipeline

on:
  push:
    branches: [ main ]

env:
  AWS_REGION: us-east-1
  CLUSTER_NAME: infoline-eks-cluster
  ECR_BACKEND_REPO: 903479130308.dkr.ecr.us-east-1.amazonaws.com/hello-springboot
  ECR_FRONTEND_PUBLIC_REPO: 903479130308.dkr.ecr.us-east-1.amazonaws.com/frontend-public
  ECR_FRONTEND_ADMIN_REPO: 903479130308.dkr.ecr.us-east-1.amazonaws.com/frontend-admin

permissions:
  id-token: write
  contents: read

jobs:
  backend:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: java-api
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-java@v3
        with:
          java-version: '17'
          distribution: 'temurin'
      - name: Build Spring Boot
        run: mvn clean package -DskipTests
      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: arn:aws:iam::903479130308:role/GitHubActionsOIDCRole
          aws-region: ${{ env.AWS_REGION }}
      - name: Login to ECR
        run: |
          aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $ECR_BACKEND_REPO
      - name: Build and Push Docker image
        run: |
          docker build -t hello-springboot:latest .
          docker tag  hello-springboot:latest $ECR_BACKEND_REPO:latest
          docker push $ECR_BACKEND_REPO:latest

  frontend-public:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: angular/frontend-public
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v3
        with:
          node-version: '20'
      - name: Install dependencies
        run: npm install
      - name: Build Angular (frontend-public)
        run: npm run build -- --configuration production --project frontend-public
      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: arn:aws:iam::903479130308:role/GitHubActionsOIDCRole
          aws-region: ${{ env.AWS_REGION }}
      - name: Login to ECR
        run: |
          aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $ECR_FRONTEND_PUBLIC_REPO
      - name: Build and Push Docker image
        run: |
          docker build -t frontend-public:latest .
          docker tag  frontend-public:latest $ECR_FRONTEND_PUBLIC_REPO:latest
          docker push $ECR_FRONTEND_PUBLIC_REPO:latest

  frontend-admin:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: angular/frontend-admin
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v3
        with:
          node-version: '20'
      - name: Install dependencies
        run: npm install
      - name: Build Angular (frontend-admin)
        run: npm run build -- --configuration production --project frontend-admin
      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: arn:aws:iam::903479130308:role/GitHubActionsOIDCRole
          aws-region: ${{ env.AWS_REGION }}
      - name: Login to ECR
        run: |
          aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $ECR_FRONTEND_ADMIN_REPO
      - name: Build and Push Docker image
        run: |
          docker build -t frontend-admin:latest .
          docker tag  frontend-admin:latest $ECR_FRONTEND_ADMIN_REPO:latest
          docker push $ECR_FRONTEND_ADMIN_REPO:latest

  deploy:
    runs-on: ubuntu-latest
    needs: [backend, frontend-public, frontend-admin]
    env:
      HEALTH_TIMEOUT: 300
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: arn:aws:iam::903479130308:role/GitHubActionsOIDCRole
          aws-region: ${{ env.AWS_REGION }}

      - name: Update kubeconfig (EKS)
        run: aws eks update-kubeconfig --region $AWS_REGION --name $CLUSTER_NAME

      # -------- attendre que l’addon EBS-CSI soit prêt --------
      - name: Wait for EBS CSI addon to be ready
        run: |
          aws eks describe-addon \
            --cluster-name $CLUSTER_NAME \
            --region $AWS_REGION \
            --addon-name aws-ebs-csi-driver \
            --query "addon.status"
          kubectl -n kube-system rollout status deployment/ebs-csi-controller --timeout=180s || true
          kubectl -n kube-system rollout status daemonset/ebs-csi-node --timeout=180s || true
          kubectl -n kube-system get pods -l app.kubernetes.io/name=aws-ebs-csi-driver -o wide || true

      # ---------- StorageClass par défaut (gp3) ----------
      - name: Apply default StorageClass (gp3)
        run: |
          kubectl apply -f terraform/monitoring/00-storageclass.yaml --validate=false
          kubectl get sc

      # ---------- Monitoring ----------
      - name: Apply monitoring namespace
        run: kubectl apply -f terraform/monitoring/00-namespace.yaml --validate=false

      - name: Apply Elasticsearch
        run: |
          set -euo pipefail
          kubectl apply -f terraform/monitoring/10-elasticsearch.yaml --validate=false

          echo "Waiting for PVC to bind..."
          for i in $(seq 1 36); do  # ~6 minutes
            phase=$(kubectl -n monitoring get pvc es-data-elasticsearch-0 -o jsonpath='{.status.phase}' 2>/dev/null || echo "")
            echo "PVC phase: ${phase:-<none>}"
            [ "$phase" = "Bound" ] && break
            sleep 10
          done

          if [ "${phase:-}" != "Bound" ]; then
            echo "::error ::PVC not Bound after waiting. Dumping diagnostics..."
            kubectl -n monitoring get pvc,pv
            kubectl -n monitoring describe pvc es-data-elasticsearch-0 || true
            kubectl get sc
            kubectl -n kube-system get pods -l app.kubernetes.io/name=aws-ebs-csi-driver -o wide
            exit 1
          fi

          echo "PVC Bound. Waiting for StatefulSet ready..."
          kubectl rollout status statefulset/elasticsearch -n monitoring --timeout=${HEALTH_TIMEOUT}s

      - name: Apply Kibana
        run: |
          set -euo pipefail
          kubectl apply -f terraform/monitoring/20-kibana.yaml --validate=false
          kubectl rollout status deployment/kibana -n monitoring --timeout=${HEALTH_TIMEOUT}s

      - name: Apply Fluent Bit
        run: |
          set -euo pipefail
          kubectl apply -f terraform/monitoring/30-fluent-bit.yaml --validate=false
          kubectl rollout status daemonset/fluent-bit -n monitoring --timeout=${HEALTH_TIMEOUT}s

      # ---------- Applications ----------
      - name: Deploy to EKS (apps)
        run: |
          set -euxo pipefail
          [ -f terraform/kubernetes/java-api-service.yaml ] && kubectl apply -f terraform/kubernetes/java-api-service.yaml --validate=false || true
          [ -f terraform/kubernetes/frontend-public-service.yaml ] && kubectl apply -f terraform/kubernetes/frontend-public-service.yaml --validate=false || true
          [ -f terraform/kubernetes/frontend-admin-service.yaml ] && kubectl apply -f terraform/kubernetes/frontend-admin-service.yaml --validate=false || true

          kubectl apply -f terraform/kubernetes/java-api-deployment.yaml --validate=false
          kubectl apply -f terraform/kubernetes/frontend-public-deployment.yaml --validate=false
          kubectl apply -f terraform/kubernetes/frontend-admin-deployment.yaml --validate=false

      - name: Wait for app rollouts
        run: |
          set -euxo pipefail
          kubectl rollout status deployment/java-api -n default --timeout=${HEALTH_TIMEOUT}s
          kubectl rollout status deployment/frontend-public -n default --timeout=${HEALTH_TIMEOUT}s
          kubectl rollout status deployment/frontend-admin -n default --timeout=${HEALTH_TIMEOUT}s

      - name: Smoke tests
        run: |
          set -euo pipefail
          ns=default
          k8s_curl () {
            svc="$1"; shift
            paths=("$@")
            if ! kubectl get svc -n "$ns" "$svc" >/dev/null 2>&1; then
              echo "::warning ::Service $svc not found -> skipping"; return 0
            fi
            ok=0
            for p in "${paths[@]}"; do
              code=$(kubectl run "curl-$svc-$$" -n "$ns" --rm -i --restart=Never \
                --image=curlimages/curl:8.10.0 --quiet -- /bin/sh -lc \
                "curl -s -o /dev/null -w '%{http_code}' http://$svc.$ns.svc.cluster.local$p || true")
              echo "$svc $p -> HTTP=$code"
              echo "$code" | grep -qE '^2..$' && ok=1 && break
            done
            if [ "$ok" -eq 0 ]; then
              echo "::warning title=Smoke test failed::$svc has no 2xx on paths: ${paths[*]}"
            fi
          }
          k8s_curl java-api-service /actuator/health /api/actuator/health /health /
          k8s_curl frontend-public-service /
          k8s_curl frontend-admin-service  /
