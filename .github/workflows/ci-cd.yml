name: CI/CD Pipeline

on:
  push:
    branches: [ main ]

env:
  AWS_REGION: us-east-1
  CLUSTER_NAME: infoline-eks-cluster
  ECR_BACKEND_REPO: 903479130308.dkr.ecr.us-east-1.amazonaws.com/hello-springboot
  ECR_FRONTEND_PUBLIC_REPO: 903479130308.dkr.ecr.us-east-1.amazonaws.com/frontend-public
  ECR_FRONTEND_ADMIN_REPO: 903479130308.dkr.ecr.us-east-1.amazonaws.com/frontend-admin

permissions:
  id-token: write
  contents: read

jobs:
  backend:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: java-api
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-java@v3
        with:
          java-version: '17'
          distribution: 'temurin'
      - name: Build Spring Boot
        run: mvn clean package -DskipTests
      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: arn:aws:iam::903479130308:role/GitHubActionsOIDCRole
          aws-region: ${{ env.AWS_REGION }}
      - name: Login to ECR
        run: |
          aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $ECR_BACKEND_REPO
      - name: Build and Push Docker image
        run: |
          docker build -t hello-springboot:latest .
          docker tag  hello-springboot:latest $ECR_BACKEND_REPO:latest
          docker push $ECR_BACKEND_REPO:latest

  frontend-public:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: angular/frontend-public
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v3
        with:
          node-version: '20'
      - name: Install dependencies
        run: npm install
      - name: Build Angular (frontend-public)
        run: npm run build -- --configuration production --project frontend-public
      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: arn:aws:iam::903479130308:role/GitHubActionsOIDCRole
          aws-region: ${{ env.AWS_REGION }}
      - name: Login to ECR
        run: |
          aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $ECR_FRONTEND_PUBLIC_REPO
      - name: Build and Push Docker image
        run: |
          docker build -t frontend-public:latest .
          docker tag  frontend-public:latest $ECR_FRONTEND_PUBLIC_REPO:latest
          docker push $ECR_FRONTEND_PUBLIC_REPO:latest

  frontend-admin:
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: angular/frontend-admin
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-node@v3
        with:
          node-version: '20'
      - name: Install dependencies
        run: npm install
      - name: Build Angular (frontend-admin)
        run: npm run build -- --configuration production --project frontend-admin
      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: arn:aws:iam::903479130308:role/GitHubActionsOIDCRole
          aws-region: ${{ env.AWS_REGION }}
      - name: Login to ECR
        run: |
          aws ecr get-login-password --region $AWS_REGION | docker login --username AWS --password-stdin $ECR_FRONTEND_ADMIN_REPO
      - name: Build and Push Docker image
        run: |
          docker build -t frontend-admin:latest .
          docker tag  frontend-admin:latest $ECR_FRONTEND_ADMIN_REPO:latest
          docker push $ECR_FRONTEND_ADMIN_REPO:latest

  deploy:
    runs-on: ubuntu-latest
    needs: [backend, frontend-public, frontend-admin]
    env:
      HEALTH_TIMEOUT: 900
    steps:
      - uses: actions/checkout@v4

      - name: Configure AWS credentials (OIDC)
        uses: aws-actions/configure-aws-credentials@v2
        with:
          role-to-assume: arn:aws:iam::903479130308:role/GitHubActionsOIDCRole
          aws-region: ${{ env.AWS_REGION }}

      - name: Update kubeconfig (EKS)
        run: aws eks update-kubeconfig --region $AWS_REGION --name $CLUSTER_NAME

      - name: Wait for EBS CSI addon to be ready (kubectl only)
        run: |
          set -euo pipefail
          for i in $(seq 1 30); do
            if kubectl -n kube-system get deploy ebs-csi-controller >/dev/null 2>&1 && \
               kubectl -n kube-system get ds ebs-csi-node >/dev/null 2>&1; then
              echo "EBS-CSI resources found."
              break
            fi
            echo "EBS-CSI resources not found yet... retry $i/30"
            sleep 10
          done
          kubectl -n kube-system rollout status deployment/ebs-csi-controller --timeout=300s || true
          kubectl -n kube-system rollout status daemonset/ebs-csi-node --timeout=300s || true
          kubectl -n kube-system get pods -l app.kubernetes.io/name=aws-ebs-csi-driver -o wide || true

      # ---------- Namespace Monitoring ----------
      - name: Apply monitoring namespace
        run: kubectl apply -f terraform/monitoring/00-namespace.yaml --validate=false

      # ---------- StorageClass gp3 (ext4) : remplacement propre ----------
      - name: Ensure StorageClass gp3 is ext4 (replace if needed)
        run: |
          set -euo pipefail
          desired_fs="ext4"
          sc="gp3"
          pvc_ns="monitoring"
          pvc_name="es-data-elasticsearch-0"

          exists=$(kubectl get sc "$sc" -o name 2>/dev/null || true)
          if [ -n "$exists" ]; then
            fs=$(kubectl get sc "$sc" -o jsonpath='{.parameters.fsType}' 2>/dev/null || echo "")
            echo "Existing StorageClass $sc with fsType='${fs:-<none>}'"
            if [ "${fs:-}" != "$desired_fs" ]; then
              echo "StorageClass $sc not '$desired_fs' -> replacing cleanly"

              # 1) Supprimer proprement ES (et son PVC) pour libérer la SC
              kubectl -n "$pvc_ns" delete statefulset elasticsearch --ignore-not-found --cascade=orphan
              kubectl -n "$pvc_ns" delete pvc "$pvc_name" --ignore-not-found

              # 2) Vérifier s'il reste d'autres PVC utilisant 'gp3'
              users=$(kubectl get pvc -A -o jsonpath='{range .items[*]}{.metadata.namespace}{" "}{.metadata.name}{" "}{.spec.storageClassName}{"\n"}{end}' | awk '$3=="gp3" {print $1 "/" $2}')
              if [ -n "$users" ]; then
                echo "::warning ::D'autres PVC utilisent encore '$sc' (ils continueront à fonctionner même si on supprime la SC) :"
                echo "$users"
              fi

              # 3) Supprimer puis recréer la SC 'gp3' au format ext4
              kubectl delete sc "$sc"
              kubectl apply -f terraform/monitoring/00-storageclass.yaml --validate=false
            else
              echo "StorageClass $sc already uses '$desired_fs' -> nothing to do"
            fi
          else
            echo "StorageClass $sc not found -> creating"
            kubectl apply -f terraform/monitoring/00-storageclass.yaml --validate=false
          fi

          kubectl get sc

      # ---------- Elasticsearch ----------
      - name: Apply Elasticsearch (StatefulSet + Service)
        run: |
          set -euo pipefail
          ns="monitoring"
          pvc="es-data-elasticsearch-0"

          kubectl apply -f terraform/monitoring/10-elasticsearch.yaml --validate=false

          echo "Waiting for PVC to bind..."
          for i in $(seq 1 36); do
            phase=$(kubectl -n "$ns" get pvc "$pvc" -o jsonpath='{.status.phase}' 2>/dev/null || echo "")
            echo "PVC phase: ${phase:-<none>}"
            [ "$phase" = "Bound" ] && break
            sleep 10
          done

          if [ "${phase:-}" != "Bound" ]; then
            echo "::error ::PVC not Bound after waiting. Dumping diagnostics..."
            kubectl -n "$ns" get pvc,pv
            kubectl -n "$ns" describe pvc "$pvc" || true
            kubectl get sc
            kubectl -n kube-system get pods -l app.kubernetes.io/name=aws-ebs-csi-driver -o wide
            exit 1
          fi

          echo "PVC Bound. Waiting for StatefulSet ready..."
          if ! kubectl rollout status statefulset/elasticsearch -n "$ns" --timeout=${HEALTH_TIMEOUT}s; then
            echo "::error ::Elasticsearch not ready within timeout. Dumping pod describe and logs..."
            kubectl -n "$ns" describe pod -l app=elasticsearch || true
            pod=$(kubectl -n "$ns" get pod -l app=elasticsearch -o jsonpath='{.items[0].metadata.name}' 2>/dev/null || echo "")
            if [ -n "$pod" ]; then
              kubectl -n "$ns" logs "$pod" --tail=200 || true
            fi
            exit 1
          fi

      # ---------- Kibana ----------
      - name: Apply Kibana
        run: |
          set -euo pipefail
          kubectl apply -f terraform/monitoring/20-kibana.yaml --validate=false
          kubectl rollout status deployment/kibana -n monitoring --timeout=${HEALTH_TIMEOUT}s

      # ---------- Fluent Bit ----------
      - name: Apply Fluent Bit
        run: |
          set -euo pipefail
          kubectl apply -f terraform/monitoring/30-fluent-bit.yaml --validate=false
          kubectl rollout status daemonset/fluent-bit -n monitoring --timeout=${HEALTH_TIMEOUT}s

      # ---------- Applications ----------
      - name: Deploy to EKS (apps)
        run: |
          set -euxo pipefail
          [ -f terraform/kubernetes/java-api-service.yaml ] && kubectl apply -f terraform/kubernetes/java-api-service.yaml --validate=false || true
          [ -f terraform/kubernetes/frontend-public-service.yaml ] && kubectl apply -f terraform/kubernetes/frontend-public-service.yaml --validate=false || true
          [ -f terraform/kubernetes/frontend-admin-service.yaml ] && kubectl apply -f terraform/kubernetes/frontend-admin-service.yaml --validate=false || true

          kubectl apply -f terraform/kubernetes/java-api-deployment.yaml --validate=false
          kubectl apply -f terraform/kubernetes/frontend-public-deployment.yaml --validate=false
          kubectl apply -f terraform/kubernetes/frontend-admin-deployment.yaml --validate=false

      - name: Wait for app rollouts
        run: |
          set -euxo pipefail
          kubectl rollout status deployment/java-api -n default --timeout=${HEALTH_TIMEOUT}s
          kubectl rollout status deployment/frontend-public -n default --timeout=${HEALTH_TIMEOUT}s
          kubectl rollout status deployment/frontend-admin -n default --timeout=${HEALTH_TIMEOUT}s

      - name: Smoke tests
        run: |
          set -euo pipefail
          ns=default
          k8s_curl () {
            svc="$1"; shift
            paths=("$@")
            if ! kubectl get svc -n "$ns" "$svc" >/dev/null 2>&1; then
              echo "::warning ::Service $svc not found -> skipping"; return 0
            fi
            ok=0
            for p in "${paths[@]}"; do
              code=$(kubectl run "curl-$svc-$$" -n "$ns" --rm -i --restart=Never \
                --image=curlimages/curl:8.10.0 --quiet -- /bin/sh -lc \
                "curl -s -o /dev/null -w '%{http_code}' http://$svc.$ns.svc.cluster.local$p || true")
              echo "$svc $p -> HTTP=$code"
              echo "$code" | grep -qE '^2..$' && ok=1 && break
            done
            if [ "$ok" -eq 0 ]; then
              echo "::warning title=Smoke test failed::$svc has no 2xx on paths: ${paths[*]}"
            fi
          }
          k8s_curl java-api-service /actuator/health /api/actuator/health /health /
          k8s_curl frontend-public-service /
          k8s_curl frontend-admin-service  /
